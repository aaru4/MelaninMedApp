{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHMAEtiCBo35Niec3VqKXG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download and Setup"
      ],
      "metadata": {
        "id": "UFFiIr9wDKiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "c1lS4qyCrRqv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px9URA3e8OOM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms as T\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds  = [0.229, 0.224, 0.225]\n",
        "test_transform = T.Compose([\n",
        "    lambda x: x.convert('RGB'),\n",
        "    T.Resize(299),\n",
        "    T.CenterCrop(299),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=means, std=stds)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "class DDI_Dataset(Dataset):\n",
        "    def __init__(self, root, csv_path=None, transform=None):\n",
        "        self.root = root\n",
        "        if csv_path is None:\n",
        "            csv_path = os.path.join(os.path.dirname(root), \"ddi_metadata.csv\")\n",
        "\n",
        "        self.annotations = pd.read_csv(csv_path)\n",
        "        self.image_files = self.annotations['DDI_file'].tolist()\n",
        "        self.transform = transform\n",
        "\n",
        "        m_key = 'malignant'\n",
        "        if m_key not in self.annotations:\n",
        "            self.annotations[m_key] = self.annotations['malignancy(malig=1)'].apply(lambda x: x == 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_name = self.image_files[index]\n",
        "        img_path = os.path.join(self.root, file_name)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        annotation = self.annotations.iloc[index]\n",
        "        target = int(annotation['malignant'])\n",
        "        skin_tone = annotation['skin_tone']\n",
        "\n",
        "        return img_path, img, target, skin_tone\n",
        "\n",
        "    def subset(self, skin_tone=None, diagnosis=None):\n",
        "        skin_tone = [12, 34, 56] if skin_tone is None else skin_tone\n",
        "        diagnosis = [\"benign\", \"malignant\"] if diagnosis is None else diagnosis\n",
        "\n",
        "        for si in skin_tone:\n",
        "            assert si in [12, 34, 56], f\"{si} is not a valid skin tone\"\n",
        "        for di in diagnosis:\n",
        "            assert di in [\"benign\", \"malignant\"], f\"{di} is not a valid diagnosis\"\n",
        "\n",
        "        # Filter the annotations based on skin tone and diagnosis\n",
        "        filtered_annotations = self.annotations[\n",
        "            (self.annotations['skin_tone'].isin(skin_tone)) &\n",
        "            (self.annotations['malignant'].isin([di == \"malignant\" for di in diagnosis]))\n",
        "        ]\n",
        "\n",
        "        indices = filtered_annotations.index.tolist()\n",
        "\n",
        "        return Subset(self, indices)\n"
      ],
      "metadata": {
        "id": "EjmloSjR9XIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root = '/content/DDI/images'\n",
        "dataset = DDI_Dataset(root=root, transform=test_transform)\n",
        "\n",
        "for i in range(5):\n",
        "    path, img, target, skin_tone = dataset[i]\n",
        "    print(f\"Image {i+1}:\")\n",
        "    print(f\"Path: {path}\")\n",
        "    print(f\"Target (Malignant: 1, Benign: 0): {target}\")\n",
        "    print(f\"Skin Tone: {skin_tone}\")\n",
        "\n",
        "    plt.imshow(img.permute(1, 2, 0))  # Permute the tensor dimensions to (H, W, C) for plotting\n",
        "    plt.title(f\"Target: {target}, Skin Tone: {skin_tone}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WiP71XMNArh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "malignant_dir = '/content/DDI/images/malignant'\n",
        "benign_dir = '/content/DDI/images/benign'\n",
        "\n",
        "os.makedirs(malignant_dir, exist_ok=True)\n",
        "os.makedirs(benign_dir, exist_ok=True)\n",
        "\n",
        "for i in range(655):\n",
        "    path, img, target, skin_tone = dataset[i]\n",
        "    file_name = os.path.basename(path)\n",
        "    if target == 1:\n",
        "        dest_dir = malignant_dir\n",
        "    else:\n",
        "        dest_dir = benign_dir\n",
        "\n",
        "    dest_path = os.path.join(dest_dir, file_name)\n",
        "    shutil.move(path, dest_path)\n",
        "    print(f\"Moved {file_name} to {dest_dir}\")\n",
        "\n",
        "print(\"Image sorting complete.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3iAS6W14O4Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "gzVK7mcwDU0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coremltools"
      ],
      "metadata": {
        "id": "nsLRvvRdwSYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define normalization parameters\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(299),\n",
        "    T.CenterCrop(299),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=means, std=stds)\n",
        "])\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Create paths for benign and malignant images\n",
        "        for label, sub_dir in enumerate(['benign', 'malignant']):\n",
        "            sub_dir_path = os.path.join(root_dir, sub_dir)\n",
        "            for filename in os.listdir(sub_dir_path):\n",
        "                if filename.lower().endswith('.png'):  # Ensure it's a .png file\n",
        "                    self.image_paths.append(os.path.join(sub_dir_path, filename))\n",
        "                    self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Load dataset and create dataloader\n",
        "root = '/content/DDI/images'\n",
        "dataset = CustomDataset(root_dir=root, transform=test_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Attention Layer\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "        value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "\n",
        "        attention = torch.bmm(query, key)  # Attention score\n",
        "        attention = nn.functional.softmax(attention, dim=-1)\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))  # Weighted sum of values\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "        return out + x  # Residual connection\n",
        "\n",
        "# Model with Attention\n",
        "class ResNetWithAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetWithAttention, self).__init__()\n",
        "        self.base_model = models.resnet18(pretrained=True)\n",
        "        self.attention = AttentionLayer(in_channels=self.base_model.fc.in_features)\n",
        "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model.conv1(x)\n",
        "        x = self.base_model.bn1(x)\n",
        "        x = self.base_model.relu(x)\n",
        "        x = self.base_model.maxpool(x)\n",
        "\n",
        "        x = self.base_model.layer1(x)\n",
        "        x = self.base_model.layer2(x)\n",
        "        x = self.base_model.layer3(x)\n",
        "        x = self.base_model.layer4(x)\n",
        "\n",
        "        x = self.attention(x)  # Apply attention\n",
        "\n",
        "        x = self.base_model.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.base_model.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = ResNetWithAttention().to(device)\n",
        "model.train()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train the model\n",
        "def train_model(dataloader, model, criterion, optimizer, epochs=13):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for imgs, lbls in dataloader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, lbls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "train_model(dataloader, model, criterion, optimizer)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'resnet_attention_model.pth')\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(dataloader, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            feature = model.base_model(imgs)  # Get features from ResNet\n",
        "            features.append(feature.cpu().numpy())\n",
        "            labels.append(lbls.numpy())\n",
        "    return np.concatenate(features), np.concatenate(labels)\n",
        "\n",
        "# Extract features and split data\n",
        "features, labels = extract_features(dataloader, model)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features and train logistic regression\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear', C=1.0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.5f}\")\n"
      ],
      "metadata": {
        "id": "BI4I_HGqWqdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = ['Benign', 'Malignant']\n",
        "\n",
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Malignancy Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Ct-rcJ7D-ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate sensitivity and specificity\n",
        "sensitivity = tp / (tp + fn)\n",
        "specificity = tn / (tn + fp)\n",
        "false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
        "print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
        "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "ZeNS9lor5ION"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define normalization parameters (same as used during training)\n",
        "means = [0.485, 0.456, 0.406]\n",
        "stds = [0.229, 0.224, 0.225]\n",
        "test_transform = T.Compose([\n",
        "    T.Resize(299),\n",
        "    T.CenterCrop(299),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=means, std=stds)\n",
        "])\n",
        "\n",
        "# Function to load a single image and preprocess it\n",
        "def load_image(image_path, transform):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Function to predict logits and probabilities\n",
        "def predict_image(image_path, model):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    image = load_image(image_path, test_transform).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(image)\n",
        "        probabilities = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
        "\n",
        "    return logits.cpu().numpy(), probabilities.cpu().numpy()\n",
        "\n",
        "# Load the saved model\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)  # Adjust final layer for binary classification\n",
        "model.load_state_dict(torch.load('resnet101_model.pth'))\n",
        "model = model.to(device)\n",
        "\n",
        "# Test with your own image\n",
        "image_path = '/content/DDI/images/malignant/000025.png'  # Replace with your image path\n",
        "logits, probabilities = predict_image(image_path, model)\n",
        "\n",
        "# Print results\n",
        "print(f\"Logits: {logits}\")\n",
        "print(f\"Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Class: {np.argmax(probabilities)}\")"
      ],
      "metadata": {
        "id": "QsWQs4pVv5ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "traceable_model = torch.jit.trace(model, load_image(image_path, test_transform).to(device))\n",
        "\n",
        "# Define the input type for Core ML conversion\n",
        "_input = ct.ImageType(\n",
        "    name=\"input_1\",\n",
        "    shape=(1, 3, 299, 299),  # Shape should match the input tensor (batch, channels, height, width)\n",
        "    bias=[-m/s for m, s in zip(means, stds)],  # Bias correction (mean/std)\n",
        "    scale=1.0/(255.0 * stds[0])  # Scale correction (1/(255 * std))\n",
        ")\n",
        "\n",
        "# Convert to Core ML\n",
        "mlmodel = ct.convert(\n",
        "    traceable_model,\n",
        "    inputs=[_input]\n",
        ")\n",
        "\n",
        "mlmodel.user_defined_metadata['description'] = \"Skin lesion classification model (ResNet18)\"\n",
        "mlmodel.user_defined_metadata['version'] = \"1.0\"\n",
        "mlmodel.short_description = \"Classifies skin lesions as benign or malignant.\"\n",
        "\n",
        "# Save the model\n",
        "mlmodel.save(\"RiskClassifier.mlpackage\")"
      ],
      "metadata": {
        "id": "HvWuQCgn6t7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Move the model to Google Drive\n",
        "!mv RiskClassifier.mlpackage/ /content/drive/My\\ Drive/"
      ],
      "metadata": {
        "id": "5RO6sv4e6umI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}